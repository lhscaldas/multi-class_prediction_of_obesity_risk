{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Prediction of Obesity Risk\n",
    "\n",
    "**Programa de Engenharia de Sistemas e Computação**\n",
    "\n",
    "**CPS833 - Data Mining**\n",
    "\n",
    "**Professor**: Geraldo Zimbrão da Silva\n",
    "\n",
    "**Aluno**: Luiz Henrique Souza Caldas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.target import FeatureCorrelation\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 dados faltantes no dataset de treino\n",
      "0 dados faltantes no dataset de teste\n",
      "0 linhas duplicadas no dataset de treino\n",
      "0 linhas duplicadas no dataset de teste\n"
     ]
    }
   ],
   "source": [
    "# Importação dos dados\n",
    "train_dataset = pd.read_csv('train.csv')\n",
    "test_dataset = pd.read_csv('test.csv')\n",
    "\n",
    "# Verificação de dados ausentes (limpeza de dados)\n",
    "print(f\"{train_dataset.isnull().any().sum()} dados faltantes no dataset de treino\")\n",
    "print(f\"{train_dataset.isnull().any().sum()} dados faltantes no dataset de teste\")\n",
    "\n",
    "# Verificação de linhas duplicadas (redução de dimensionalidade)\n",
    "print(f\"{train_dataset.duplicated().sum()} linhas duplicadas no dataset de treino\")\n",
    "print(f\"{train_dataset.duplicated().sum()} linhas duplicadas no dataset de teste\")\n",
    "\n",
    "# TODO Verificação de ouliers (redução de dimensionalidade)\n",
    "\n",
    "# Codificando features categóricas com Label Encoder (transformação de dados)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "train_dataset_encoded = train_dataset.copy()  # Faz uma cópia do dataset de treino para evitar alterações no original\n",
    "for col in train_dataset.columns:\n",
    "    if train_dataset[col].dtype == 'object':  # Verifica se a coluna é categórica\n",
    "        train_dataset_encoded[col] = label_encoder.fit_transform(train_dataset[col])\n",
    "\n",
    "test_dataset_encoded = test_dataset.copy() # Faz uma cópia do dataset de teste para evitar alterações no original\n",
    "for col in test_dataset.columns:\n",
    "    if test_dataset[col].dtype == 'object':  # Verifica se a coluna é categórica\n",
    "        test_dataset_encoded[col] = label_encoder.fit_transform(test_dataset[col])\n",
    "\n",
    "# Separação entre features e labels no dataset de treinamento e remoção da coluna id nos dois datasets\n",
    "features = train_dataset_encoded.iloc[:,1:17].values # features do dataset de treino removendo o id\n",
    "labels = train_dataset_encoded.iloc[:,17].values # labels\n",
    "test = test_dataset_encoded.drop(columns=['id']) # removendo o id do dataset de teste\n",
    "\n",
    "# Escalonando os dados (transformação de dados)\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "test = scaler.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleção de Modelo\n",
    "### Escolha da técnica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lhsca\\OneDrive\\Documentos\\Estudos\\Mestrado\\Disciplinas\\Data Mining\\Trabalho 1\\.conda\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "resultados_naive_bayes = []\n",
    "resultados_logistica = []\n",
    "resultados_forest = []\n",
    "resultados_knn = []\n",
    "resultados_svm = []\n",
    "\n",
    "for i in range(30):\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=i)\n",
    "    \n",
    "    naive_bayes = GaussianNB() # criação do modelo Naive Bayes\n",
    "    scores = cross_val_score(naive_bayes, features, labels, cv=kfold) # validação cruzada do modelo Naive Bayes\n",
    "    resultados_naive_bayes.append(scores.mean()) # avaliação do modelo Naive Bayes\n",
    "    \n",
    "    logistica = LogisticRegression(max_iter=300) # criação do modelo de Regressão Logística\n",
    "    scores = cross_val_score(logistica, features, labels, cv=kfold) # treinamento do modelo de Regressão Logística\n",
    "    resultados_logistica.append(scores.mean()) # avaliação do modelo de Regressão Logística\n",
    "\n",
    "    random_forest = RandomForestClassifier(n_jobs=-1, random_state=i) # criação do modelo de Random Forest\n",
    "    scores = cross_val_score(random_forest, features, labels, cv=kfold) # treinamento do modelo de Random Forest\n",
    "    resultados_forest.append(scores.mean()) # avaliação do modelo de Random Forest\n",
    "\n",
    "    knn = KNeighborsClassifier() # criação do modelo de k-NN \n",
    "    scores = cross_val_score(knn, features, labels, cv=kfold) # treinamento do modelo de k-NN \n",
    "    resultados_knn.append(scores.mean()) # avaliação do modelo de k-NN \n",
    "\n",
    "    svm = SVC() # criação do modelo de SVM\n",
    "    scores = cross_val_score(svm, features, labels, cv=kfold) # treinamento do modelo de SVM\n",
    "    resultados_svm.append(scores.mean()) # avaliação do modelo de SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO rede neural com tensor flow e GPU\n",
    "resultados_rede_neural = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Naïve Bayes com média {np.mean(resultados_naive_bayes)} e desvio padrão {np.std(resultados_naive_bayes)}\")\n",
    "print(f\"Regressão Logística com média {np.mean(resultados_logistica)} e desvio padrão {np.std(resultados_logistica)}\")\n",
    "print(f\"Random Forest com média {np.mean(resultados_forest)} e desvio padrão {np.std(resultados_forest)}\")\n",
    "print(f\"k-NN com média {np.mean(resultados_knn)} e desvio padrão {np.std(resultados_knn)}\")\n",
    "print(f\"SVM com média {np.mean(resultados_svm)} e desvio padrão {np.std(resultados_svm)}\")\n",
    "print(f\"Rede Neural com média {np.mean(resultados_rede_neural)} e desvio padrão {np.std(resultados_rede_neural)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A técnica Random Forest obteve a maior média, então mesmo com seu desvio padrão sendo o maior, foi a técnica selecionada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimização dos hiperparâmetros com validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 800 candidates, totalling 8000 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m\n\u001b[0;32m      3\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_features\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m],\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m600\u001b[39m, \u001b[38;5;241m700\u001b[39m, \u001b[38;5;241m800\u001b[39m, \u001b[38;5;241m900\u001b[39m, \u001b[38;5;241m1000\u001b[39m]\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m     10\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator \u001b[38;5;241m=\u001b[39m RandomForestClassifier(), param_grid \u001b[38;5;241m=\u001b[39m param_grid, \n\u001b[0;32m     11\u001b[0m                           cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(x_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\lhsca\\OneDrive\\Documentos\\Estudos\\Mestrado\\Disciplinas\\Data Mining\\Trabalho 1\\.conda\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lhsca\\OneDrive\\Documentos\\Estudos\\Mestrado\\Disciplinas\\Data Mining\\Trabalho 1\\.conda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\lhsca\\OneDrive\\Documentos\\Estudos\\Mestrado\\Disciplinas\\Data Mining\\Trabalho 1\\.conda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\lhsca\\OneDrive\\Documentos\\Estudos\\Mestrado\\Disciplinas\\Data Mining\\Trabalho 1\\.conda\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    918\u001b[0m         clone(base_estimator),\n\u001b[0;32m    919\u001b[0m         X,\n\u001b[0;32m    920\u001b[0m         y,\n\u001b[0;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    927\u001b[0m     )\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    931\u001b[0m     )\n\u001b[0;32m    932\u001b[0m )\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lhsca\\OneDrive\\Documentos\\Estudos\\Mestrado\\Disciplinas\\Data Mining\\Trabalho 1\\.conda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\lhsca\\OneDrive\\Documentos\\Estudos\\Mestrado\\Disciplinas\\Data Mining\\Trabalho 1\\.conda\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\lhsca\\OneDrive\\Documentos\\Estudos\\Mestrado\\Disciplinas\\Data Mining\\Trabalho 1\\.conda\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lhsca\\OneDrive\\Documentos\\Estudos\\Mestrado\\Disciplinas\\Data Mining\\Trabalho 1\\.conda\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, stratify = labels)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30, 40, None],\n",
    "    'max_features': [2, 3, 4, 5],\n",
    "    'min_samples_leaf': [1, 2, 3, 5],\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = RandomForestClassifier(), param_grid = param_grid, \n",
    "                          cv = 10, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Melhores parâmetros {grid_search.best_params_}\")\n",
    "print(f\"Melhor score {grid_search.best_score_} no conjunto de treinamento\")\n",
    "\n",
    "random_forest = grid_search.best_estimator_\n",
    "y_predict = random_forest.predict(x_test)\n",
    "resultado = accuracy_score(y_test, y_predict)\n",
    "print(f\" Score no conjunto de teste {resultado}\")\n",
    "\n",
    "print(f\"O ganho após a otimização foi de {(np.mean(resultados_forest)-resultado)*100:.4f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação do modelo usando o dataset de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, stratify = labels)\n",
    "    \n",
    "naive_bayes = GaussianNB() # criação do modelo Naive Bayes\n",
    "naive_bayes.fit(x_train,y_train) # treinamento do modelo Naive Bayes\n",
    "y_predict_naive_bayes = naive_bayes.predict(x_test) # classificação com o modelo Naive Bayes\n",
    "resultado_naive_bayes = accuracy_score(y_test, y_predict_naive_bayes) # percentual de acerto do modelo Naive Beyes\n",
    "\n",
    "random_forest = RandomForestClassifier(max_depth=2000, max_features=5, min_samples_leaf=2, n_estimators=700, n_jobs=-1) # criação do modelo de Random Forest\n",
    "random_forest.fit(x_train,y_train) # treinamento do modelo de Random Forest\n",
    "y_predict_forest = random_forest.predict(x_test) # classificação com o modelo Random Forest\n",
    "resultado_forest = accuracy_score(y_test, y_predict_forest) # percentual de acerto do modelo Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliação dos resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodificando os labels\n",
    "mapeamento = {\n",
    "    0: \"Insufficient_Weight\",\n",
    "    1: \"Normal_Weight\",\n",
    "    2: \"Obesity_Type_I\",\n",
    "    3: \"Obesity_Type_II\",\n",
    "    4: \"Obesity_Type_III\",\n",
    "    5: \"Overweight_Level_I\",\n",
    "    6: \"Overweight_Level_II\"\n",
    "}\n",
    "y_predict_naive_bayes = [mapeamento[i] for i in y_predict_naive_bayes]\n",
    "y_predict_forest = [mapeamento[i] for i in y_predict_forest]\n",
    "y_test = [mapeamento[i] for i in y_test]\n",
    "\n",
    "cm_nb = confusion_matrix(y_test, y_predict_naive_bayes)\n",
    "cm_rf = confusion_matrix(y_test, y_predict_forest)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "sns.heatmap(cm_nb, annot=True, cmap='Blues', fmt='d', ax=axs[0])\n",
    "axs[0].set_title('Matriz de Confusão para o modelo Naïve Beyes')\n",
    "axs[0].set_xlabel('Predicted labels')\n",
    "axs[0].set_ylabel('True labels')\n",
    "sns.heatmap(cm_rf, annot=True, cmap='Blues', fmt='d', ax=axs[1])\n",
    "axs[1].set_title('Matriz de Confusão para o modelo Random Forest')\n",
    "axs[1].set_xlabel('Predicted labels')\n",
    "axs[1].set_ylabel('True labels')\n",
    "\n",
    "print(f\"A acurácia do Naïve Beyes foi de {resultado_naive_bayes}\")\n",
    "print(f\"A acurácia do Random Forest foi de {resultado_forest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação do dataset de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(max_depth=2000, max_features=5, min_samples_leaf=2, n_estimators=700, n_jobs=-1) # criação do modelo de Random Forest\n",
    "random_forest.fit(features,labels) # treinamento do modelo de Random Forest com o dataset de treinamento completo\n",
    "y_predict_forest = random_forest.predict(test) # classificação com o modelo Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profundidade Média das Árvores: 24.467142857142857\n",
      "Profundidade Máxima das Árvores: 33\n"
     ]
    }
   ],
   "source": [
    "# Acesse os estimadores (árvores) individuais\n",
    "estimadores = random_forest\n",
    "\n",
    "# Examine a profundidadede cada árvore\n",
    "profundidade_arvores = [estimador.tree_.max_depth for estimador in estimadores]\n",
    "\n",
    "# Calculando o tamanho médio e máximo das árvores\n",
    "profundidade_media = sum(profundidade_arvores) / len(profundidade_arvores)\n",
    "profundidade_maxima = max(profundidade_arvores)\n",
    "\n",
    "print(\"Profundidade Média das Árvores:\", profundidade_media)\n",
    "print(\"Profundidade Máxima das Árvores:\", profundidade_maxima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geração do CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapeamento = {\n",
    "    0: \"Insufficient_Weight\",\n",
    "    1: \"Normal_Weight\",\n",
    "    2: \"Obesity_Type_I\",\n",
    "    3: \"Obesity_Type_II\",\n",
    "    4: \"Obesity_Type_III\",\n",
    "    5: \"Overweight_Level_I\",\n",
    "    6: \"Overweight_Level_II\"\n",
    "}\n",
    "predict_categorico = [mapeamento[i] for i in y_predict_forest]\n",
    "resultado = np.column_stack((test_dataset['id'].values, predict_categorico))\n",
    "np.savetxt('resuldado.csv', resultado, delimiter=',', header=\"id,NObeyesdad\", fmt='%s', comments='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
